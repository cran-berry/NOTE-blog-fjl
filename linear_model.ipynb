{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model\n",
    "$$\\theta = (XX^T)^{-1}Xy$$\n",
    "$$y = \\theta_0 + x_1\\theta_1 + \\cdots + x_n\\theta_n = X^T \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "num = 1000\n",
    "X = np.random.rand(num, 2)\n",
    "y = 2 + 3 * X[:,0] + 4 * X[:,1] + np.random.randn(num)\n",
    "y = y.reshape((num, 1))\n",
    "X_b = np.c_[np.ones((num, 1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()  # 直接求解矩阵， 求逆\n",
    "lin_reg.fit(X, y.ravel())\n",
    "print(lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent: BGD\n",
    "eta = 0.1\n",
    "theta = np.random.randn(3, 1)\n",
    "n_iter = 1000\n",
    "for _ in range(n_iter):\n",
    "    gradient = 1 / num * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradient\n",
    "print(theta)\n",
    "\n",
    "# Stochastic Gradient Descent: SGD\n",
    "from sklearn.linear_model import SGDRegressor, ARDRegression, \n",
    "# penalty: 在loss后面加上参数的惩罚项, eta0: 初始学习率, tol: 误差, max_iter: 最大迭代次数\n",
    "sgd_reg = SGDRegressor(penalty='l2', eta0=0.01, max_iter=1000, tol=1e-3)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "print(lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "num = 1000\n",
    "X = 6 * np.random.rand(num, 1) - 3\n",
    "# X = np.random.rand(num, 1)  # 图像不一样\n",
    "y = 0.5 * X ** 2 + X + 1 + np.random.randn(num, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolynomialFeatures\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)  # include_bias: 偏置常数项\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "# LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y.ravel())\n",
    "print(lin_reg.intercept_, lin_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_learning_curves(reg, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    train_error, test_error = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        reg.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = reg.predict(X_train[:m])\n",
    "        y_val_predict = reg.predict(X_test)\n",
    "        train_error.append(mean_squared_error(y_train_predict, y_train[:m]))\n",
    "        test_error.append(mean_squared_error(y_val_predict, y_test))\n",
    "    plt.plot(np.sqrt(train_error), \"r-\", linewidth=3, label=\"train\")\n",
    "    plt.plot(np.sqrt(test_error), \"b--\", linewidth=3, label=\"test\")\n",
    "    plt.ylim(0,2)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "# plot_learning_curves(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "polynomial_regression = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"lin_reg\", LinearRegression(n_jobs=-1))\n",
    "])\n",
    "plot_learning_curves(polynomial_regression, X, y)\n",
    "# polynomial_regression.fit(X, y)\n",
    "# polynomial_regression.named_steps[\"lin_reg\"].intercept_,  # .coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Linear Models 正则化线性模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression（L2正则化）\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y.ravel())\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\", max_iter=5, tol=None)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression（L1正则化）\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y.ravel())\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\", max_iter=5, tol=None)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic_net.fit(X, y.ravel())\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"elasticnet\", alpha=0.1, l1_ratio=0.5)\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提前中断训练 warm_start\n",
    "from sklearn.base import clone\n",
    "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
    "minimum_val_error = float(\"inf\") #正无穷\n",
    "best_epoch, best_model = None, None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train) # 继续训练\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val_predict, y_val)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg) #保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic回归（Logistic Regression）\n",
    "# 在线性回归模型的基础上增加了sigmoid函数, 损失函数采用对数似然损失函数\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_reg = LogisticRegression()\n",
    "# logistic_reg.fit(X, y)\n",
    "\n",
    "# Softmax回归（Softmax Regression）\n",
    "# Softmax回归计算每一类的一个概率，归为概率最大的一类, 损失函数使用交叉熵\n",
    "softmax_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", C=10)\n",
    "# softmax_reg.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
