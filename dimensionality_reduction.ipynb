{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "## 维数灾难\n",
    "##### 例如，在单位正方形内随机选点，那么随机选的点离所有边界大于0.001的概率为0.4%，但在一个10000维单位超立方体，这个概率大于99 999999%。因此高维超立方体中的大多数随机点都非常接近边界。\n",
    "##### 在单位超立方体随机选两个点，在2维空间，两个点之间平均距离约为0.52；在3维空间约为0.66，在1,000,000维空间，则约为408.25，因此在高维空间，这很可能会使得训练集和测试集相差很大，导致训练过拟合。\n",
    "##### 理论上，只要通过增加训练集，就能达到训练空间足够密集。但是随着维度的增加，需要的训练集是呈指数增长的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA 基础, SVD 分解\n",
    "# numpy\n",
    "x1 = np.random.randn(100)\n",
    "x2 = x1 + np.random.rand(100)\n",
    "X = np.stack([x1, x2], axis=1)\n",
    "X_centered = X - np.mean(X, axis=1, keepdims=True)  # PCA假设数据以原点为中心，Scikit-learn中的PCA类已经减去均值\n",
    "u, s, v = np.linalg.svd(X_centered)\n",
    "d = 1\n",
    "X_D = X_centered.dot(v.T[:, :d])  # 降维后数据\n",
    "\n",
    "# sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "X_D = pca.fit_transform(X)\n",
    "pca.components_, pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择合理的维数\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum <= 0.95) + 1\n",
    "\n",
    "pca = PCA(n_components=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 增量PCA（IPCA）\n",
    "from sklearn.datasets import fetch_mldata\n",
    "digits = fetch_mldata(\"MNIST original\")\n",
    "X = digits[\"data\"]\n",
    "# IncrementalPCA\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "n_batches = 100\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_D = inc_pca.transform(X)\n",
    "# memmap, batch_size\n",
    "def helper(filename, m, n, n_batches):\n",
    "    X = np.memmap(filename, mode=\"r\", dtype=np.float64, shape=(m, n))\n",
    "    batch_size = m // n_batches\n",
    "    inc_pca = IncrementalPCA(n_components=154, batch_size = batch_size)\n",
    "    inc_pca.fit(X)\n",
    "\n",
    "# 随机PCA\n",
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_D = rnd_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核PCA(Kernel PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2)\n",
    "fig = plt.figure(figsize=(15,6))\n",
    "fig.add_subplot(1, 2, 1, projection='3d')\n",
    "plt.scatter(X[:,0], X[:,1], X[:,2], c=t)\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_D = kpca.fit_transform(X)\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.scatter(X_D[:,0], X_D[:,1], c=t)\n",
    "# 此方法要使用大量内存，可能会使内存溢出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid 选择合适的核与参数\n",
    "from sklearn.datasets import fetch_mldata\n",
    "digits = fetch_mldata(\"MNIST original\")\n",
    "X, y = digits[\"data\"][:8000].astype(np.float64), digits[\"target\"][:8000]\n",
    "# X, y = digits[\"data\"].astype(np.float64), digits[\"target\"]\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "clf = Pipeline([\n",
    "    (\"std\", StandardScaler()),\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_reg\", LogisticRegression(solver=\"lbfgs\", C=2, multi_class=\"multinomial\"))\n",
    "])\n",
    "param_grid = [\n",
    "    {\"kpca__kernel\": [\"rbf\", \"sigmoid\"], \"kpca__gamma\": np.linspace(0.001, 0.5, 50)}\n",
    "]\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 局部线性嵌入（Locally Linear Embedding）LLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "X, y = make_swiss_roll(1000, noise=0.0)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "ax = fig.add_subplot(121, projection=\"3d\")\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=y)\n",
    "# LocallyLinearEmbedding\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "lle = LocallyLinearEmbedding(n_neighbors=10, n_components=2, n_jobs=-1)\n",
    "X_D = lle.fit_transform(X)\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(X_D[:,0], X_D[:,1], c=y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
